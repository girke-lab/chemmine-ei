.TH man 1 "16 Aug 2012" "1.0" "ei.py"

.SH NAME
ei.py \- Fast search of large chemical compound sets

.SH SYNOPSIS
ei.py --init compounds [-m measure]

ei.py -r R -d D [options] [accuracy | indexed | clean sdfname]

ei.py --embed  -r R -d D [options]

.SH DESCRIPTION

.SS Initialization
An initial compound database must be created with the following command:

  $ ei.py --init my.sdf

SDF is supported by default. Other formats can be supported through customization,
see the below section. It might complain if your SDF does not follow the SDF
specification. If this happens, try use OpenBabel to convert it to SMILES
and back to SDF. This will create  a folder called 'data'. Ei.py should always be
executed in the folder containing this directory (ie, the parent directory of "data").

Check ~/.eirc and make appropriate changes. One important setting is
whether to use qsub. If you do not have access to qsub-style job queue,
set the 'processor' value to 'bash' to run the job locally.

If you want to set variables for just the local directory, you can modify 
data/eirc, which will be read after ~/.eirc and override any variables
set there.

.SS Creating a Searchable Database

Invoke ei.py with two required arguments, the number of reference
compounds and the number of dimensions. For example:

  $ ei.py -r 300 -d 100 

Ei.py will be verbose and keep updating you
with what it is currently busy with. For example, you might see

.nf
  checking database size...
  processing query 300
  processing query 22906
  150128.girkelab-59-200.ucr.edu
  150129.girkelab-59-200.ucr.edu
  qsub jobs still running. --Tue Aug 24 22:47:34 2010  
.fi

Upon this, your job has been submitted to the cluster for actual
embedding. ei.py will watch the cluster for the progress using qstat, and
will continue its next step when it identifies that the cluster has
finished its job.

ei.py does its job in a job folder, named after the number of reference
compounds and the number of embedding dimensions. For example, using 300
reference compounds to generate 300-dimensional embedding (-r 300 -d
100) will result in a job folder run-300-100. 

If you are eager to know the status, simply see how many lines have been
generated by the embedder. Each line corresponds to one compound that
has been processed successfully. For example:

.nf
  $ cd run-300-100
  $ wc -l 300-100-*.out
		7962 300-100-1.out
		2906 300-100-2.out
	  10868 total
.fi

You can compare this number to the number of lines in main.iddb to have
an idea of the progress.

When the embedding is finished, ei.py will merge the results generated
by different cluster jobs and convert the data to binary format. The
screen may look like

.nf
  merging coordinate results
  total time in solving puzzle: 4574.019
  making binary version of coordinate files...
  4220730 lines processed in 1.4071 seconds
  4198824 lines processed in 0.063864 seconds
.fi     

If ei.py crashes or gets terminated for any reason and you need to rerun
it, it will try to reuse data only if you invoke the existing-input
mode. The existing-input mode is started by adding the *.cdb file in the
run-*-* folder. For example:

	$ ei.py -r 300 -d 100 run-300-100/b0780f55853624ec274713648531e003.cdb

This will allow ei.py to find intermediate files and pick up from where
it stopped. If you to not specify the *.cdb and run

	$ ei.py -r 300 -d 100

again after crash, then it will generate a new set of compounds as
reference compounds and restart the embedding process. This is useful
when you need to try a new set of reference compounds.

The embedding result is the file matrix.<R>.<D>, where <R> is the number
of references and <D> is the number of dimensions. In the above example,
the output would be run-300-100/matrix.300.100.


.SS Performance Evaluation
To evaluate the performance you an run:

	$ ei.py -r 300 -d 100 accuracy

Ei.py will then perform a performance evaluation by testing the
embedding results in similarity search. The way this works is by
approximating 1,000 random similarity searches (determined by
data/test_queries.iddb) by nearest neighbor search using the coordinates
from the embedding results. The search results are then compared to the
reference search results (chemical-search.results.gz). 

The comparison results are summarized in two type of files. The first
type lists the recall for different k value, k being the number of
numbers to retrieve. For example, if the recall is 70% for top-100
compound search - 70 of the 100 results are among the real top-100
compounds - then the value at line 100 is 0.7. Several relaxation ration
is used, each generating a file in this form. For instance,
recall.ratio-10 is the file listing the recalls when relaxation
ratio is 10. The other file, recall.csv, lists recalls of different
relaxation ratios in one file by limiting to selected k value. In this
CSV file, the rows correspond to different relaxation ratios, and the
columns are different k values. You will be able to pick an appropriate
relaxation ratio for the k values you are interested in.

To test the performance of LSH-assisted search, run

	$ ei.py -r 300 -d 100 indexed

The result will be in run-300-100/indexed.performance. It's a 1,000-line
recall values. Each line corresponds to one test query.

LSH search performance is highly sensitive to your LSH parameter. The
default parameter is listed in ~/.eirc

	lsh_param = " -W 1.39564 -M 19 -L 30 -K 600 -S 30 -T 30 "

When you have your embedding result in a matrix file, you should
follow instruction on

	http://lshkit.sourceforge.net/dd/d2a/mplsh-tune_8cpp.html

To find the best values for these parameters



.SS COMPOUND DATABASE
    
EI works on compound databases. A compound database consists of two files.
One is the actual database data file in some format. This database, known
as the <bindb>, typically consists of actual compound data. For example,
an atom-pair database would use a <bindb> to store atom pair descriptors.
The other file is an ID database or <iddb>. This database is just a text
file, each line of which is a sequence ID used to subset the <bindb>. For
example, if the <bindb> contains 1,000 compounds, then the <iddb> can
contain at most 1,000 lines of sequential numbers from 1 to 1000. Using
this <bindb> and <iddb>, then we have a compound database for 1,000
compounds. If, for example, the <iddb> contains 500 lines of odd numbers
from 1 up to 999, then the combination of the <bindb> and the <iddb>
describes a compound database of 500 compounds. Note that <iddb> uses
1-based numbering, not 0-based. 

You may see these files under the /data folder:

	- main.iddb : this is the main iddb. Compounds referenced in this 
iddb will be embedded.

	- test_query.iddb : this file is auto-generated. It is a subset of 
main.iddb used to perform automatic search tests in order to evaluate 
the embedding quality.

Note that there is no requirement on what format <bindb> uses. EI does not
utilize <bindb> directly. Instead, it passes the path to <bindb> to
the program bin/db2db_distance. If you use your own custom similarity
measure (see CUSTOMIZATION below), as long as your custom
bin/db2db_distance program can understand <bindb>, then it is fine. If you
hardcode the path to your own compound database in your custom
bin/db2db_distance, you might not create a <bindb> at all. 
    
.SS CUSTOMIZATION

EI allows you to plug in your own similarity measurement. In order to do
this, you will have to provide an implementation for the following
programs in binary or script form.

There are two ways to tell EI about your own measures. The first is to set the 
full path of each binary in the configuration file (.eirc) using the variables
DB2DB_DISTANCE, DB_BUILDER, and DB_SUBSET. The second method is to put your 
binaries in a directory on the PATH (e.g. /usr/local/bin/), and name them 
like ei-db2db_distance.my_measure, where the first part is one of the below 3 
binaries and the second part is the name of your measure. Then you can call
ei.py and ode.py with "-m my_measure". This method will be used below, but
either method will work. When you run ei.py --init -m my_measure ..., the 
measure used will be stored in the data/eirc file so that future operations 
automatically use the correct programs.

.B ei-db2db_distance

You need to write a binary or script that calculate database-to-database
distance matrix, which supports the following two syntax forms:

  db2db_distance.my_measure chem.db 1.iddb 2.iddb

  db2db_distance.my_measure chem.db chem2.db

where chem.db is the path to <bindb> in, for example, the /data folder,
1.iddb is the first <iddb> and 2.iddb is the second <iddb> (see above for
definition of <bindb> and <iddb>). Your program must generate a distance
matrix for these two databases. The output should be lines of whitespace separated
numbers. Each line should contain the distances of the first element of
the first db against each member of the second db.  Name your program as
db2db_distance.my_measure and then you can invoke EI as, for example, 

  $ ei.py -r 300 -d 100 -m my_measure

to use your similarity measure.

.B ei-db_builder

This program takes compounds in whatever format you want to support and
generates a database in the format you want to support. The syntax you 
need to support is:

  db_builder.my_measure input.compounds output.db

.B ei-db_subset

This program takes the database format you support and a 1-based index file
and generate a sub-database for compounds listed in the index file. 
Db_subset must support the following syntax

  db_subset.my_measure original.db index.file output.db

.SH OPTIONS
.TP 5
-r NUM
number of reference compounds to use
.TP
-d NUM
number of dimensions
.TP 
-m STRING
similarity measure to use
.TP
-q FILE
query file in sdf format
.TP
-x FILE
reference cdb file to use. This will usually be a file in the "run"
directory and start with a hash string. e.g.: 
run-300-100/b0780f55853624ec274713648531e003.cdb
.TP
-v
verbose
.TP
--vv
very verbose
.TP
--dry-run
don't actually create a searchable database, but print out what
commands would  have been executed
.TP
-s, --slice=NUM
number of compounds to process on each node
.TP
--init
create an initial compound database
.TP
--embed
print out embedded coordinates of query
.TP 
--help
print help message

.TP
accuracy
perform accuracy analysis only using existing data

.TP 
indexed
perform accuracy analysis between indexed and non indexed results

.TP 
clean
remove all files starting with sdfname as well as a few other temp files



.SH BUGS
No known bugs.

.SH AUTHOR
Eddie Cao
